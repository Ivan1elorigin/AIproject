{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b375ab7-13e1-4f60-899e-466ae9a3ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c0357bb-7614-490a-9b18-53c5b639041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d744c139-a10a-4961-bae7-d2bf74e4ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c86e00c-82d5-4405-9dc0-292d6042607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)]\n",
      "NLTK: 3.8.1\n",
      "Scikit-learn: 1.4.2\n",
      "Pandas: 2.2.2\n",
      "NumPy: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pd.__version__))\n",
    "print('NumPy: {}'.format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfdfe766-ee88-4287-b4b3-44bf020e6654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47adf1a2-0a44-4f7f-a9f9-1447323a1718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e782099a-d181-47a9-9aa4-00b06671178b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',\n",
       "       'class', 'tweet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "596fac56-cad7-485d-928a-1a9b4dfb5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'] ,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485961fc-7b2a-4482-8877-ff7dec1009ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80812846-af56-4b7b-8c5a-c8280298e213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class    0\n",
       "tweet    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd8fa65-7987-4453-972a-8bb17d62adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   class   24783 non-null  int64 \n",
      " 1   tweet   24783 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 387.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7105e22-d969-4561-9056-1a8a7e973c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa526146-3159-4402-b487-5f15b2c7530a",
   "metadata": {},
   "source": [
    "<strong>class:</strong> class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5522f78a-8e90-477b-a848-f43c31707e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[\"tweet\"]\n",
    "text[:10]\n",
    "\n",
    "label = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f75a8c-701a-4b99-9d0f-b9d79ba999b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar '!!!' al inicio. Eliminar 'RT'. Sustituir nombres de usuario\n",
    "# text = text.str.replace(r'^!+', '', regex=True).str.replace(r'^RT\\s+', '', regex=True).str.replace(r'@\\w+', 'users_name', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1476f292-f7a7-46ae-b20a-b62c98785115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar '!!!' al inicio. Eliminar 'RT' correctamente. Sustituir nombres de usuario. Eliminar espacios en blanco sobrantes\n",
    "# processed = text.str.replace(r'^!+', '', regex=True).str.replace(r'^RT\\s+', '', regex=True).str.replace(r'@\\w+', 'users_name', regex=True).str.strip()                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feda239c-5cac-4321-aef1-f8848626ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar '!!!' al inicio. Eliminar 'RT' correctamente en cualquier parte del texto. Sustituir nombres de usuario. Eliminar espacios en blanco sobrantes\n",
    "processed = text .str.replace(r'^!+', '', regex=True).str.replace(r'\\bRT\\b\\s+', '', regex=True).str.replace(r'@\\w+', 'name', regex=True).str.strip()                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cef3f94-ee20-4dae-bd3c-36a48dde6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ', regex=True)\n",
    "\n",
    "# Replace whitespace between terms with a single space\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eff12867-5c73-46b6-860a-6061edd76090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        users_name  as a woman you shouldn t complain ...\n",
       "1        users_name  boy dats cold   tyga dwn bad for c...\n",
       "2        users_name dawg     users_name  you ever fuck ...\n",
       "3            users_name  users_name she look like a tranny\n",
       "4        users_name  the shit you hear about me might b...\n",
       "                               ...                        \n",
       "24778    you s a muthaf   in lie   8220 users_name  use...\n",
       "24779    you ve gone and broke the wrong heart baby  an...\n",
       "24780    young buck wanna eat     dat nigguh like i ain...\n",
       "24781                youu got wild bitches tellin you lies\n",
       "24782      ruffled   ntac eileen dahlia   beautiful col...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = processed.str.lower()\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9322afc9-029c-4acc-8997-3fff0051d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c591e451-f4c4-41a6-9189-39e84257b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53922b89-fd3c-47b7-9239-565eb1605231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        users_nam woman complain clean hous amp man al...\n",
       "1        users_nam boy dat cold tyga dwn bad cuffin dat...\n",
       "2        users_nam dawg users_nam ever fuck bitch start...\n",
       "3                     users_nam users_nam look like tranni\n",
       "4        users_nam shit hear might true might faker bit...\n",
       "                               ...                        \n",
       "24778    muthaf lie 8220 users_nam users_nam users_nam ...\n",
       "24779      gone broke wrong heart babi drove redneck crazi\n",
       "24780    young buck wanna eat dat nigguh like aint fuck...\n",
       "24781                       youu got wild bitch tellin lie\n",
       "24782    ruffl ntac eileen dahlia beauti color combin p...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d09276-e5eb-4ae2-8826-c7af82d5bd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 19438\n",
      "Most common words: [('users_nam', 19272), ('bitch', 11480), ('hoe', 4352), ('128514', 3241), ('http', 3127), ('co', 3013), ('like', 2873), ('fuck', 2269), ('pussi', 2267), ('nigga', 2019), ('get', 1782), ('8220', 1726), ('8221', 1682), ('got', 1612), ('ass', 1598)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# Print the result\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd3a83fa-4ec4-4afd-b0eb-3d29f4679997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 1500 most common words as features\n",
    "word_features = [x[0] for x in all_words.most_common(1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c918b58d-78a4-4dd2-8f9e-08b6789529ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['users_nam',\n",
       " 'bitch',\n",
       " 'hoe',\n",
       " '128514',\n",
       " 'http',\n",
       " 'co',\n",
       " 'like',\n",
       " 'fuck',\n",
       " 'pussi',\n",
       " 'nigga',\n",
       " 'get',\n",
       " '8220',\n",
       " '8221',\n",
       " 'got',\n",
       " 'ass',\n",
       " 'shit',\n",
       " 'u',\n",
       " '8230',\n",
       " 'trash',\n",
       " 'lol',\n",
       " 'amp',\n",
       " 'go',\n",
       " 'know',\n",
       " 'love',\n",
       " 'look',\n",
       " 'one',\n",
       " 'want',\n",
       " 'make',\n",
       " 'girl',\n",
       " 'say',\n",
       " 'na',\n",
       " 'call',\n",
       " '128557',\n",
       " 'yo',\n",
       " 'bird',\n",
       " 'talk',\n",
       " 'man',\n",
       " 'bad',\n",
       " 'think',\n",
       " 'need',\n",
       " 'faggot',\n",
       " 'hate',\n",
       " 'good',\n",
       " 'see',\n",
       " 'time',\n",
       " 'ya',\n",
       " 'still',\n",
       " 'back',\n",
       " 'let',\n",
       " 'im',\n",
       " 'day',\n",
       " 'never',\n",
       " 'come',\n",
       " 'gon',\n",
       " 'peopl',\n",
       " 'realli',\n",
       " 'real',\n",
       " 'retard',\n",
       " 'right',\n",
       " 'even',\n",
       " 'would',\n",
       " 'take',\n",
       " 'white',\n",
       " 'tell',\n",
       " 'eat',\n",
       " 'lmao',\n",
       " 'said',\n",
       " '128553',\n",
       " 'dick',\n",
       " 'tri',\n",
       " 'wit',\n",
       " 'stop',\n",
       " 'wan',\n",
       " 'play',\n",
       " 'give',\n",
       " 'bout',\n",
       " 'damn',\n",
       " 'nigger',\n",
       " 'ta',\n",
       " 'littl',\n",
       " 'da',\n",
       " 'niggah',\n",
       " 'dat',\n",
       " 'gt',\n",
       " 'feel',\n",
       " 'life',\n",
       " '128175',\n",
       " '2',\n",
       " 'caus',\n",
       " 'n',\n",
       " 'dont',\n",
       " 'yanke',\n",
       " 'cunt',\n",
       " 'new',\n",
       " 'fag',\n",
       " 'nicca',\n",
       " 'charli',\n",
       " 'everi',\n",
       " 'put',\n",
       " 'alway',\n",
       " 'money',\n",
       " '8217',\n",
       " 'better',\n",
       " 'ghetto',\n",
       " 'game',\n",
       " 'lil',\n",
       " '65039',\n",
       " 'boy',\n",
       " 'hit',\n",
       " 'tweet',\n",
       " 'use',\n",
       " 'ever',\n",
       " '128530',\n",
       " 'yellow',\n",
       " 'dumb',\n",
       " 'twitter',\n",
       " 'fuckin',\n",
       " 'aint',\n",
       " 'ugli',\n",
       " 'thing',\n",
       " 'keep',\n",
       " 'guy',\n",
       " 'ur',\n",
       " 'stupid',\n",
       " 'tho',\n",
       " 'big',\n",
       " 'work',\n",
       " 'watch',\n",
       " 'mean',\n",
       " 'yall',\n",
       " 'way',\n",
       " '3',\n",
       " 'black',\n",
       " 'start',\n",
       " 'ask',\n",
       " 'ho',\n",
       " 'year',\n",
       " 'fat',\n",
       " 'today',\n",
       " 'much',\n",
       " 'yeah',\n",
       " 'friend',\n",
       " 'color',\n",
       " 'kill',\n",
       " 'face',\n",
       " 'old',\n",
       " '1',\n",
       " 'well',\n",
       " 'mad',\n",
       " 'em',\n",
       " 'act',\n",
       " 'name',\n",
       " 'smh',\n",
       " 'head',\n",
       " 'oh',\n",
       " 'someon',\n",
       " 'could',\n",
       " 'around',\n",
       " 'made',\n",
       " 'last',\n",
       " 'turn',\n",
       " 'cuz',\n",
       " 'show',\n",
       " 'side',\n",
       " 'gone',\n",
       " 'best',\n",
       " 'night',\n",
       " 'suck',\n",
       " 'babi',\n",
       " 'happi',\n",
       " '128525',\n",
       " 'dude',\n",
       " 'someth',\n",
       " 'live',\n",
       " 'us',\n",
       " 'school',\n",
       " 'shut',\n",
       " 'kid',\n",
       " 'first',\n",
       " 'next',\n",
       " 'tryna',\n",
       " 'mani',\n",
       " 'anoth',\n",
       " 'monkey',\n",
       " 'sex',\n",
       " 'beat',\n",
       " 'follow',\n",
       " 'bro',\n",
       " 'thank',\n",
       " '128128',\n",
       " 'text',\n",
       " 'wear',\n",
       " 'crazi',\n",
       " 'find',\n",
       " 'two',\n",
       " 'care',\n",
       " 'god',\n",
       " 'b',\n",
       " 'women',\n",
       " 'stay',\n",
       " 'die',\n",
       " 'w',\n",
       " 'noth',\n",
       " 'femal',\n",
       " 'wait',\n",
       " 'haha',\n",
       " 'gay',\n",
       " 'told',\n",
       " 'hell',\n",
       " 'fight',\n",
       " 'broke',\n",
       " '5',\n",
       " '9733',\n",
       " 'walk',\n",
       " 'bruh',\n",
       " 'that',\n",
       " 'phone',\n",
       " 'lie',\n",
       " '4',\n",
       " 'nig',\n",
       " 'hope',\n",
       " 'high',\n",
       " 'run',\n",
       " 'seen',\n",
       " 'trust',\n",
       " 'swear',\n",
       " 'mom',\n",
       " 'thought',\n",
       " 'word',\n",
       " 'hous',\n",
       " 'pic',\n",
       " 'loyal',\n",
       " 'wish',\n",
       " 'tonight',\n",
       " 'browni',\n",
       " 'buy',\n",
       " 'done',\n",
       " 'throw',\n",
       " '128079',\n",
       " 'home',\n",
       " 'song',\n",
       " 'ima',\n",
       " 'cute',\n",
       " 'pleas',\n",
       " 'nah',\n",
       " 'car',\n",
       " 'cut',\n",
       " 'wrong',\n",
       " 'everybodi',\n",
       " '12288',\n",
       " 'bring',\n",
       " 'lt',\n",
       " 'cracker',\n",
       " 'ye',\n",
       " '128564',\n",
       " 'hard',\n",
       " 'cri',\n",
       " 'funni',\n",
       " 'lot',\n",
       " 'alreadi',\n",
       " 'pretti',\n",
       " 'might',\n",
       " 'wtf',\n",
       " 'hair',\n",
       " 'basic',\n",
       " 'leav',\n",
       " 'hot',\n",
       " 'miss',\n",
       " 'son',\n",
       " 'oreo',\n",
       " '128527',\n",
       " 'long',\n",
       " 'world',\n",
       " 'di',\n",
       " 'straight',\n",
       " 'free',\n",
       " 'sorri',\n",
       " 'cheat',\n",
       " 'red',\n",
       " '9996',\n",
       " 'pull',\n",
       " 'redneck',\n",
       " 'sure',\n",
       " 'fake',\n",
       " 'nobodi',\n",
       " 'sleep',\n",
       " 'win',\n",
       " 'sound',\n",
       " '128076',\n",
       " 'woman',\n",
       " 'smoke',\n",
       " 'tf',\n",
       " 'went',\n",
       " '10',\n",
       " 'mock',\n",
       " 'rememb',\n",
       " 'though',\n",
       " 'job',\n",
       " 'dog',\n",
       " 'fan',\n",
       " 'anyth',\n",
       " 'guess',\n",
       " 'jihadi',\n",
       " 'person',\n",
       " 'hey',\n",
       " '128129',\n",
       " 'hand',\n",
       " 'sinc',\n",
       " 'young',\n",
       " 'stfu',\n",
       " 'eye',\n",
       " 'dem',\n",
       " 'problem',\n",
       " 'team',\n",
       " 'dyke',\n",
       " 'chick',\n",
       " 'type',\n",
       " 'bet',\n",
       " 'week',\n",
       " 'cool',\n",
       " 'away',\n",
       " 'nigguh',\n",
       " 'ju',\n",
       " 'ok',\n",
       " '9995',\n",
       " 'cant',\n",
       " 'pictur',\n",
       " 'lmfao',\n",
       " '128588',\n",
       " 'r',\n",
       " 'chang',\n",
       " 'stand',\n",
       " 'parti',\n",
       " 'pay',\n",
       " 'everyon',\n",
       " 'full',\n",
       " 'queer',\n",
       " 'move',\n",
       " 'drop',\n",
       " '128064',\n",
       " 'top',\n",
       " 'singl',\n",
       " 'pop',\n",
       " 'break',\n",
       " 'sit',\n",
       " 'nice',\n",
       " 'probabl',\n",
       " 'wife',\n",
       " 'everyth',\n",
       " 'great',\n",
       " 'ladi',\n",
       " 'teabagg',\n",
       " '128109',\n",
       " 'whole',\n",
       " 'point',\n",
       " '128526',\n",
       " 'men',\n",
       " 'ball',\n",
       " 'weed',\n",
       " 'mind',\n",
       " 'catch',\n",
       " '6',\n",
       " 'left',\n",
       " 'half',\n",
       " 'ion',\n",
       " 'date',\n",
       " 'fact',\n",
       " 'line',\n",
       " 'actual',\n",
       " 'mouth',\n",
       " 'yea',\n",
       " 'took',\n",
       " 'bc',\n",
       " '128563',\n",
       " 'pass',\n",
       " 'music',\n",
       " 'favorit',\n",
       " 'post',\n",
       " 'bae',\n",
       " 'af',\n",
       " 'believ',\n",
       " 'end',\n",
       " 'bodi',\n",
       " 'yet',\n",
       " 'coon',\n",
       " 'mayb',\n",
       " 'must',\n",
       " 'laugh',\n",
       " 'club',\n",
       " 'hear',\n",
       " 'birthday',\n",
       " 'chill',\n",
       " 'open',\n",
       " 'wonder',\n",
       " 'light',\n",
       " 'send',\n",
       " 'dead',\n",
       " 'drink',\n",
       " 'girlfriend',\n",
       " '128536',\n",
       " 'hurt',\n",
       " 'omg',\n",
       " 'either',\n",
       " 'video',\n",
       " 'els',\n",
       " 'without',\n",
       " 'listen',\n",
       " 'ex',\n",
       " 'main',\n",
       " 'lose',\n",
       " 'food',\n",
       " 'class',\n",
       " 'drive',\n",
       " 'okay',\n",
       " 'outta',\n",
       " 'dress',\n",
       " 'worri',\n",
       " 'brown',\n",
       " 'pick',\n",
       " 'iphon',\n",
       " 'place',\n",
       " 'lost',\n",
       " 'porn',\n",
       " 'slap',\n",
       " 'differ',\n",
       " 'reason',\n",
       " 'morn',\n",
       " 'fun',\n",
       " 'somebodi',\n",
       " 'sole',\n",
       " 'save',\n",
       " 'ride',\n",
       " '128074',\n",
       " 'wow',\n",
       " 'saw',\n",
       " 'read',\n",
       " 'happen',\n",
       " 'respect',\n",
       " 'booti',\n",
       " 'nasti',\n",
       " 'via',\n",
       " 'sell',\n",
       " '128515',\n",
       " 'tire',\n",
       " 'idk',\n",
       " 'shoot',\n",
       " 'heart',\n",
       " 'boyfriend',\n",
       " 'gave',\n",
       " 'cat',\n",
       " 'racist',\n",
       " 'number',\n",
       " 'check',\n",
       " 'came',\n",
       " 'sick',\n",
       " 'readi',\n",
       " 'also',\n",
       " 'thot',\n",
       " 'smell',\n",
       " 'rt',\n",
       " '0',\n",
       " 'rather',\n",
       " 'coupl',\n",
       " 'relationship',\n",
       " 'kick',\n",
       " 'obama',\n",
       " 'ill',\n",
       " 'block',\n",
       " 'goe',\n",
       " 'true',\n",
       " 'movi',\n",
       " 'bag',\n",
       " 'drunk',\n",
       " 'dirti',\n",
       " 'help',\n",
       " 'gettin',\n",
       " 'dad',\n",
       " 'talkin',\n",
       " '128520',\n",
       " 'shot',\n",
       " 'pregnant',\n",
       " 'naw',\n",
       " 'fine',\n",
       " 'c',\n",
       " '128540',\n",
       " '1041204',\n",
       " 'cold',\n",
       " 'ratchet',\n",
       " 'lookin',\n",
       " 'hold',\n",
       " 'mine',\n",
       " 'heard',\n",
       " 'lame',\n",
       " 'earli',\n",
       " 'treat',\n",
       " 'bed',\n",
       " 'quit',\n",
       " '7',\n",
       " 'none',\n",
       " 'busi',\n",
       " 'meet',\n",
       " 'finna',\n",
       " 'ha',\n",
       " 'tast',\n",
       " 'togeth',\n",
       " 'rich',\n",
       " 'water',\n",
       " 'flappi',\n",
       " '160',\n",
       " 'enough',\n",
       " 'outsid',\n",
       " 'support',\n",
       " 'speak',\n",
       " 'voic',\n",
       " 'annoy',\n",
       " 'front',\n",
       " 'nd',\n",
       " 'sometim',\n",
       " 'slut',\n",
       " 'state',\n",
       " 'ppl',\n",
       " 'anyway',\n",
       " 'kiss',\n",
       " '128545',\n",
       " 'least',\n",
       " 'imma',\n",
       " 'folk',\n",
       " 'self',\n",
       " 'anyon',\n",
       " 'sad',\n",
       " '128299',\n",
       " 'street',\n",
       " 'found',\n",
       " 'wet',\n",
       " 'understand',\n",
       " 'sexi',\n",
       " 'ah',\n",
       " '128166',\n",
       " 'wake',\n",
       " 'liter',\n",
       " 'instagram',\n",
       " 'season',\n",
       " 'month',\n",
       " 'beauti',\n",
       " 'negro',\n",
       " 'redskin',\n",
       " 'joke',\n",
       " 'close',\n",
       " 'citi',\n",
       " 'niggaz',\n",
       " 'yu',\n",
       " 'seem',\n",
       " 'ignor',\n",
       " 'tranni',\n",
       " 'leg',\n",
       " '128529',\n",
       " 'twat',\n",
       " 'american',\n",
       " '128524',\n",
       " 'piss',\n",
       " 'may',\n",
       " 'forget',\n",
       " 'tomorrow',\n",
       " 'nip',\n",
       " 'fall',\n",
       " 'famili',\n",
       " 'mother',\n",
       " 'almost',\n",
       " '9',\n",
       " 'nude',\n",
       " 'seriou',\n",
       " 'brother',\n",
       " 'bye',\n",
       " '128581',\n",
       " 'weekend',\n",
       " 'cop',\n",
       " 'dream',\n",
       " 'low',\n",
       " '100',\n",
       " 'mf',\n",
       " 'thirsti',\n",
       " 'ago',\n",
       " '20',\n",
       " 'realiz',\n",
       " 'clean',\n",
       " 'round',\n",
       " 'grow',\n",
       " 'scare',\n",
       " 'america',\n",
       " 'suppos',\n",
       " 'karma',\n",
       " 'ape',\n",
       " 'room',\n",
       " 'matter',\n",
       " 'door',\n",
       " 'drug',\n",
       " 'quick',\n",
       " 'hella',\n",
       " '8',\n",
       " 'set',\n",
       " 'fli',\n",
       " 'attent',\n",
       " 'android',\n",
       " 'xxx',\n",
       " 'minut',\n",
       " 'deserv',\n",
       " 'second',\n",
       " 'dm',\n",
       " 'sum',\n",
       " 'asian',\n",
       " '128080',\n",
       " 'weak',\n",
       " 'till',\n",
       " 'kno',\n",
       " 'unless',\n",
       " 'uncl',\n",
       " 'late',\n",
       " 'instead',\n",
       " '127814',\n",
       " 'met',\n",
       " 'k',\n",
       " 'g',\n",
       " 'final',\n",
       " 'ipad',\n",
       " 'slope',\n",
       " 'ate',\n",
       " 'truth',\n",
       " 'hang',\n",
       " 'attitud',\n",
       " 'blow',\n",
       " '30',\n",
       " 'hood',\n",
       " 'hahaha',\n",
       " 'roll',\n",
       " 'bunch',\n",
       " 'shirt',\n",
       " 'perfect',\n",
       " 'ice',\n",
       " 'cloth',\n",
       " 'skinni',\n",
       " '24',\n",
       " 'bore',\n",
       " 'becom',\n",
       " 'finger',\n",
       " 'rap',\n",
       " 'learn',\n",
       " 'complain',\n",
       " 'claim',\n",
       " 'alon',\n",
       " 'stori',\n",
       " 'argu',\n",
       " 'nothin',\n",
       " 'whore',\n",
       " 'skin',\n",
       " 'rock',\n",
       " 'answer',\n",
       " 'park',\n",
       " 'gun',\n",
       " 'social',\n",
       " 'whatev',\n",
       " 'everyday',\n",
       " 'lmaoo',\n",
       " 'later',\n",
       " 'homi',\n",
       " 'blue',\n",
       " 'soon',\n",
       " 'fire',\n",
       " 'sing',\n",
       " '128131',\n",
       " 'steal',\n",
       " 'anymor',\n",
       " 'sweet',\n",
       " 'smack',\n",
       " 'sister',\n",
       " 'kind',\n",
       " 'hillbilli',\n",
       " 'prolli',\n",
       " '11',\n",
       " 'isi',\n",
       " 'tom',\n",
       " 'short',\n",
       " 'kinda',\n",
       " 'countri',\n",
       " 'dawg',\n",
       " 'momma',\n",
       " 'p',\n",
       " 'daddi',\n",
       " 'past',\n",
       " 'weird',\n",
       " 'knew',\n",
       " 'public',\n",
       " 'order',\n",
       " 'race',\n",
       " 'player',\n",
       " 'far',\n",
       " 'tcot',\n",
       " 'rest',\n",
       " '128513',\n",
       " 'stick',\n",
       " 'hour',\n",
       " 'lip',\n",
       " 'lick',\n",
       " 'glad',\n",
       " 'aye',\n",
       " 'mama',\n",
       " 'chicken',\n",
       " '128548',\n",
       " 'jump',\n",
       " '128533',\n",
       " 'beer',\n",
       " 'huh',\n",
       " 'behind',\n",
       " 'chase',\n",
       " 'whip',\n",
       " 'tl',\n",
       " 'la',\n",
       " 'part',\n",
       " 'deep',\n",
       " 'cream',\n",
       " 'bullshit',\n",
       " '128532',\n",
       " 'unfollow',\n",
       " 'nut',\n",
       " 'thru',\n",
       " 'thick',\n",
       " 'teacher',\n",
       " '128584',\n",
       " 'blunt',\n",
       " 'shoe',\n",
       " 'expect',\n",
       " 'hungri',\n",
       " 'avi',\n",
       " 'marri',\n",
       " 'green',\n",
       " 'mess',\n",
       " 'knock',\n",
       " 'dey',\n",
       " 'flip',\n",
       " 'ig',\n",
       " 'deal',\n",
       " '128075',\n",
       " 'ol',\n",
       " 'lmaooo',\n",
       " 'control',\n",
       " 'fucc',\n",
       " 'mr',\n",
       " 'media',\n",
       " 'jesu',\n",
       " '128567',\n",
       " '12',\n",
       " 'count',\n",
       " 'level',\n",
       " 'bomb',\n",
       " 'crist',\n",
       " 'halloween',\n",
       " 'bar',\n",
       " 'boss',\n",
       " 'neck',\n",
       " 'j',\n",
       " 'titti',\n",
       " 'star',\n",
       " 'id',\n",
       " 'tha',\n",
       " 'jeter',\n",
       " 'proud',\n",
       " 'asshol',\n",
       " 'worth',\n",
       " 'goin',\n",
       " '10084',\n",
       " 'biggest',\n",
       " 'swag',\n",
       " 'fri',\n",
       " 'rick',\n",
       " 'piec',\n",
       " 'scream',\n",
       " 'fam',\n",
       " 'mention',\n",
       " 'handl',\n",
       " 'friday',\n",
       " 'strong',\n",
       " 'dollar',\n",
       " 'smile',\n",
       " 'lucki',\n",
       " 'cooki',\n",
       " 'step',\n",
       " 'sign',\n",
       " 'town',\n",
       " 'f',\n",
       " 'touch',\n",
       " 'less',\n",
       " '1st',\n",
       " 'worst',\n",
       " 'retweet',\n",
       " 'jail',\n",
       " 'wat',\n",
       " 'easi',\n",
       " 'sheen',\n",
       " 'soul',\n",
       " 'middl',\n",
       " '128528',\n",
       " 'summer',\n",
       " 'poor',\n",
       " 'natur',\n",
       " 'joe',\n",
       " 'trip',\n",
       " 'sayin',\n",
       " 'danc',\n",
       " 'serious',\n",
       " 'your',\n",
       " 'taken',\n",
       " 'fuzzi',\n",
       " 'clam',\n",
       " '2014',\n",
       " 'doe',\n",
       " 'beaner',\n",
       " 'trick',\n",
       " 'special',\n",
       " 'cook',\n",
       " 'playin',\n",
       " 'drake',\n",
       " 'punch',\n",
       " 'thug',\n",
       " 'chees',\n",
       " 'photo',\n",
       " 'baddest',\n",
       " 'box',\n",
       " 'fast',\n",
       " 'question',\n",
       " 'grade',\n",
       " 'vote',\n",
       " 'cock',\n",
       " 'rate',\n",
       " 'de',\n",
       " 'rape',\n",
       " 'definit',\n",
       " 'parent',\n",
       " 'stuck',\n",
       " 'confus',\n",
       " 'power',\n",
       " 'push',\n",
       " 'excus',\n",
       " 'tv',\n",
       " 'anim',\n",
       " 'hide',\n",
       " 'forev',\n",
       " 'death',\n",
       " 'dope',\n",
       " 'butt',\n",
       " 'al',\n",
       " 'enjoy',\n",
       " 'gener',\n",
       " 'account',\n",
       " 'fool',\n",
       " 'million',\n",
       " 'law',\n",
       " 'crow',\n",
       " 'bust',\n",
       " 'bone',\n",
       " 'plan',\n",
       " 'tip',\n",
       " 'mexican',\n",
       " 'test',\n",
       " 'beef',\n",
       " '128293',\n",
       " 'hi',\n",
       " 'e',\n",
       " 'super',\n",
       " 'slow',\n",
       " 'giant',\n",
       " '128073',\n",
       " 'caught',\n",
       " '50',\n",
       " 'hop',\n",
       " 'trap',\n",
       " 'rais',\n",
       " 'forgot',\n",
       " '99',\n",
       " 'nake',\n",
       " 'share',\n",
       " 'seat',\n",
       " 'shi',\n",
       " '128555',\n",
       " 'h',\n",
       " 'chug',\n",
       " 'wast',\n",
       " 'chri',\n",
       " 'scari',\n",
       " 'sport',\n",
       " 'babe',\n",
       " '128523',\n",
       " 'den',\n",
       " 'trippin',\n",
       " 'surpris',\n",
       " 'human',\n",
       " 'complet',\n",
       " 'ring',\n",
       " 'actin',\n",
       " 'dri',\n",
       " 'blame',\n",
       " 'tight',\n",
       " 'wack',\n",
       " 'rose',\n",
       " 'fit',\n",
       " 'slit',\n",
       " 'throat',\n",
       " 'feet',\n",
       " 'total',\n",
       " 'hairi',\n",
       " 'liber',\n",
       " 'weather',\n",
       " 'wild',\n",
       " 'presid',\n",
       " 'nope',\n",
       " '128522',\n",
       " 'train',\n",
       " 'disrespect',\n",
       " 'bill',\n",
       " 'republican',\n",
       " 'vs',\n",
       " 'ran',\n",
       " 'doubl',\n",
       " 'chocol',\n",
       " '15',\n",
       " 'kim',\n",
       " 'teeth',\n",
       " 'uh',\n",
       " 'rapper',\n",
       " 'write',\n",
       " 'base',\n",
       " 'rip',\n",
       " 'whitey',\n",
       " 'facebook',\n",
       " 'democrat',\n",
       " 'ugh',\n",
       " 'delet',\n",
       " 'heat',\n",
       " 'hat',\n",
       " 'lay',\n",
       " 'term',\n",
       " '8212',\n",
       " 'stress',\n",
       " 'bit',\n",
       " 'punk',\n",
       " 'fuc',\n",
       " 'paper',\n",
       " 'mane',\n",
       " 'chanc',\n",
       " 'john',\n",
       " 'choke',\n",
       " 'nation',\n",
       " 'twerk',\n",
       " 'quot',\n",
       " '187',\n",
       " 'dark',\n",
       " 'gold',\n",
       " 'news',\n",
       " 'draft',\n",
       " 'comment',\n",
       " 'crib',\n",
       " 'except',\n",
       " 'card',\n",
       " 'ebola',\n",
       " 'texa',\n",
       " 'small',\n",
       " 'cours',\n",
       " 'honestli',\n",
       " 'zebra',\n",
       " 'fo',\n",
       " 'offend',\n",
       " 'fell',\n",
       " 'idgaf',\n",
       " 'blood',\n",
       " 'pig',\n",
       " 'lebron',\n",
       " 'twinki',\n",
       " 'murder',\n",
       " 'pimp',\n",
       " 'futur',\n",
       " '128591',\n",
       " 'yesterday',\n",
       " 'lock',\n",
       " 'colleg',\n",
       " 'period',\n",
       " 'celebr',\n",
       " 'grandma',\n",
       " 'cake',\n",
       " 'fa',\n",
       " 'baker',\n",
       " 'mo',\n",
       " 'didnt',\n",
       " 'finish',\n",
       " 'three',\n",
       " 'min',\n",
       " 'king',\n",
       " 'slide',\n",
       " 'extra',\n",
       " 'dare',\n",
       " 'ga',\n",
       " 'selfi',\n",
       " 'massag',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bdecfe-56b3-4443-adc5-914a3f7df820",
   "metadata": {},
   "source": [
    "Guardamos nuestras word_features en un fichero pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83d6a923-21a6-4ff1-91ec-063fd512dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_features.pkl', 'wb') as f:\n",
    "    pickle.dump(word_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "543b5c88-4389-478a-b5b1-3228c780d0db",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_features\u001b[38;5;241m.\u001b[39mtype\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb1ef13c-6941-401a-ac64-4998ab21763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05874cc0-cbba-4ad5-86af-d4d9838d8063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_nam\n",
      "trash\n",
      "amp\n",
      "man\n",
      "take\n",
      "alway\n",
      "hous\n",
      "woman\n",
      "clean\n",
      "complain\n"
     ]
    }
   ],
   "source": [
    "features = find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcd146bc-53ec-4f32-855c-ee04c4468cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('users_nam', True),\n",
       " ('bitch', False),\n",
       " ('hoe', False),\n",
       " ('128514', False),\n",
       " ('http', False),\n",
       " ('co', False),\n",
       " ('like', False),\n",
       " ('fuck', False),\n",
       " ('pussi', False),\n",
       " ('nigga', False)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e8049b4-8aed-41cc-8bf4-d6667534243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = list(zip(processed, label))\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# Call find_features function for each SMS message\n",
    "feature_set = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e27f737c-aa14-4303-94e9-e2fd0751d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training, test = train_test_split(feature_set, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "774b53b8-90aa-4fdd-b548-a5a87bfea989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18587\n",
      "6196\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee7302-7969-4f4f-a3a0-2a2de59434cc",
   "metadata": {},
   "source": [
    "## Scikit-learn Classifier with NLTK\n",
    "Now, we build the training and test set, we can build machine learning model in scikit-learn. We are using the following alogithms and see the performance of each ones,\n",
    "\n",
    "- KNearestNeighbors\n",
    "\n",
    "- Random Forest\n",
    "- Decision Tree\n",
    "\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b919b1d4-4bc4-4725-bf06-fb5576e7587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Naive Bayes']\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MultinomialNB()\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c30d277f-ac70-4d9f-951b-3ad2c9842207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x296eded9480>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b6f5961-1db0-4d3c-92f4-a671dcb658e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors model Accuracy: 0.8653970303421562\n",
      "Decision Tree model Accuracy: 0.8710458360232408\n",
      "Random Forest model Accuracy: 0.8999354422207876\n",
      "Naive Bayes model Accuracy: 0.8897675919948353\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, test)\n",
    "    print(\"{} model Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed40cec5-656a-4d63-8f62-8ea5dfd36a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier model Accuracy: 0.8917043253712073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Since VotingClassifier can accept list type of models\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators=models, voting='hard', n_jobs=-1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_ensemble, test)\n",
    "print(\"Voting Classifier model Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c657a514-f16c-47aa-b6c1-eada0354067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features, labels = zip(*test)\n",
    "prediction = nltk_ensemble.classify_many(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90ef2fdf-21d8-4775-9119-68ca31510cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.31      0.35       350\n",
      "           1       0.93      0.94      0.94      4847\n",
      "           2       0.83      0.84      0.84       999\n",
      "\n",
      "    accuracy                           0.89      6196\n",
      "   macro avg       0.72      0.70      0.71      6196\n",
      "weighted avg       0.88      0.89      0.89      6196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8d5a06e-7f31-4a7e-84f8-26c92a927f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neither</th>\n",
       "      <th>ofensive language</th>\n",
       "      <th>hate speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neither</th>\n",
       "      <td>109</td>\n",
       "      <td>201</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ofensive language</th>\n",
       "      <td>143</td>\n",
       "      <td>4575</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hate speech</th>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         predicted                              \n",
       "                           neither ofensive language hate speech\n",
       "actual neither                 109               201          40\n",
       "       ofensive language       143              4575         129\n",
       "       hate speech              13               145         841"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame( confusion_matrix(labels, prediction),\n",
    "             index=[['actual', 'actual', 'actual'], ['neither', 'ofensive language', 'hate speech']],\n",
    "             columns = [['predicted', 'predicted', 'predicted'], ['neither', 'ofensive language', 'hate speech']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4a98a96-0d20-444c-9db3-900c8e2b8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    \"k_nearest_neighbors\": \"knn_model.pkl\",\n",
    "    \"decision_tree\": \"decision_tree_model.pkl\",\n",
    "    \"random_forest\": \"random_forest_model.pkl\",\n",
    "    \"naive_bayes\": \"naive_bayes_model.pkl\",\n",
    "    \"voting_classifier\": \"voting_classifier.pkl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98e2da17-8cd9-4896-a152-3a243e1b59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(models, voting_classifier):\n",
    "    for name, model in models:\n",
    "        with open(model_paths[name.lower().replace(\" \", \"_\")], 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    with open(model_paths[\"voting_classifier\"], 'wb') as f:\n",
    "        pickle.dump(voting_classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0129967-614e-4c56-99f0-bd989d1e57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_save = list(zip(names, classifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5abff0fb-f315-497d-835e-69f9fd3fd6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(models_to_save, nltk_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35512a44-3eb6-4fe6-81cb-583802367284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para cargar los modelos:\n",
    "model_paths = {\n",
    "    \"k_nearest_neighbors\": \"knn_model.pkl\",\n",
    "    \"decision_tree\": \"decision_tree_model.pkl\",\n",
    "    \"random_forest\": \"random_forest_model.pkl\",\n",
    "    \"naive_bayes\": \"naive_bayes_model.pkl\",\n",
    "    \"voting_classifier\": \"voting_classifier.pkl\"\n",
    "}\n",
    "\n",
    "def load_models():\n",
    "    loaded_models = {}\n",
    "    for name, path in model_paths.items():\n",
    "        with open(path, 'rb') as f:\n",
    "            loaded_models[name] = pickle.load(f)\n",
    "    return loaded_models\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
